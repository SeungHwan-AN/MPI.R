[참고사항]
1. /etc/sysconfig
	-> 리눅스 환경에서는 /etc/network/interfaces
2. /etc/hosts 는 동일
3. error 발생 = sudo : unable to resolve host [hostname]
	-> hostname과 sudo vi /etc/hostname의 이름을 동일하게 수정해준다
	-> hostname : hostname 확인 명령어
4. ubuntu 몇 비트 버전인지 확인 (openmpi-bin 설치 시 확인 필요)
	-> dpkg -s libc6 | grep Arch
5. user계정의 mpich2만 현재 삭제하고 설치 진행
	-> sudo apt-get purge --auto-remove mpich2
	-> dpeltms79(원래 설치 계정)은 별도 삭제 진행하지 않음
6. mpiuser 계정 : 무쓸모...이 계정 대신 user 계정으로 mpi 진행
	-> mpiuser / 123456qwer!
	-> 이름 : stat_mpiuser
7. mpicc를 사용할때 = /usr/bin/mpicc 의 full path 사용!
8. mpirun를 사용할때 = /usr/bin/mpirun 의 full path 사용!
9. mpicc의 compile 경로와 mpirun을 실행시킬 파일의 경로를 일치시키는 것이 중요!

[해야할일] - http://mpitutorial.com/tutorials/
1. create a shared library for R to use(for compile C functions)
	-> makefile...?
2. master->slave : ssh 작동 But slave->master : ssh error 발생 = 해결안해도 무방???
	-> 한방향으로만 했는데 hello.c 예제 정상 작동
	-> 또한 stat04가 rank0으로 master 처럼 작동
3. NFS 환경 구축... 반드시 필요??? = 우리가 LAN 환경인지 확인 필요
4. 데이터를 받아오기 위해서는 어떻게??? = NFS 환경과 관련있는지?
5. R 예제코드 실행해보기
6. 각 node마다 2개 이상의 processor를 실행시키도록 구현
	-> hostfile을 적절하게 수정하는 것이 필요
	-> 각각 사용가능한 cpu의 개수를 지정 필요
7. parameter를 던져주면 서로 다른 값을 return 하도록 구현@@@@@
	-> random number generator...?
8. dopar 반복사용 확인
	-> 반복사용 가능!
9. 서버 1개 더 연결해서 openMPI, ssh, host 설정 등 최종체크하기 = github에 정리해서 올리기!!!!!@@@@@
	-> 서버 확장 필요!!!!!
10. dsa 암호 삭제 불필요?
	-> 삭제 안해도 정상적으로 실행은 됨...
11. R 코드 예제 성공!!!
12. doMPI를 이용한 간단한 예제 실행
	->  Error in library("doMPI") : there is no package called ‘doMPI’
	: (해결)
		export LD_LIBRARY_PATH=/home/user/R/x86_64-pc-linux-gnu-library/3.5:$LD_LIBRARY_PATH
			-> 이거 모든 node에 동일하게 수행해줘야됨!!!
			(동일한 경로로 패키지 설치)
	-> shared library 문제...?
	-> R 패키지를 C 언어로 wrapping하기
13. stat04에서 slave spawn이 안됨...갑자기 -> 해결
	-> 모든 node의 Rmpi package가 설치된 library가 일치되어야 한다.
	-> .libPath()를 이용해서 모든 node의 Rmpi library path를 모두 동일하게 설정해 주었다
	-> 이처럼 모든 master와 slave들이 최대한 동일한 설정과 경로를 가지고 있어야 원할하게 작동 가능
14. mpirun으로 돌릴 수 있는 r코드 짜기
	-> 그냥 아무 r 코드나 돌리면 parallel 코딩 가능

[최종 과제]
1) doMPI로 ADMM을 구현할 수  있는 기본 틀 완성 -> 보류 -> 필요 없음
2) 평범한 R 코드를 C로 wrapping하여 완성 -> 보류 -> 필요 없을듯
3) Rmpi 이용하여 간단한 send, recv 코드 구현해보기!!!!! -> 기본 예제 작성 필요@@@@@
4) 각 node별로 지정된 개수만의 slave를 spawn시키기*****
	-> PBS
	-> process allocation
	-> (해결)
		mpi.spawn.Rslaves(nslaves = mpi.universe.size() - 1)
			-> R코드에서의 mpi.universe.size가 host_file에서 설정한 slots의 수가 된다!!!!!!
		user@stathdfs05:~$ cat host_file
		stathdfs05 slots=5 -> master용 1개 cpu 제외
		stat04 slots=8
			-> slave를 12개 spawn하도록 설정
		(결과)
		user@stathdfs05:~$ mpirun --hostfile host_file -n 1 R --slave -f /home/user/rmpi_test.R
			-> 여기서 -n 1은 실행시키고자 하는 코드(프로그램)을 master & slave 그룹을 이용해 몇번 반복할 것인지 지정한다
			-> 따라서 1로 값을 설정해주어 프로그램을  1번만 수행하고, 이때 그룹은 master 1명과 그외 slave들로 이루어진다
        12 slaves are spawned successfully. 0 failed.
master  (rank 0 , comm 1) of size 13 is running on: stathdfs05
slave1  (rank 1 , comm 1) of size 13 is running on: stathdfs05
slave2  (rank 2 , comm 1) of size 13 is running on: stathdfs05
slave3  (rank 3 , comm 1) of size 13 is running on: stathdfs05
... ... ...
slave11 (rank 11, comm 1) of size 13 is running on: stat04
slave12 (rank 12, comm 1) of size 13 is running on: stat04
$slave1
[1] "I am 1 of 13 stathdfs05"

$slave2
[1] "I am 2 of 13 stathdfs05"

$slave3
[1] "I am 3 of 13 stathdfs05"

$slave4
[1] "I am 4 of 13 stathdfs05"

$slave5
[1] "I am 5 of 13 stat04"

$slave6
[1] "I am 6 of 13 stat04"

$slave7
[1] "I am 7 of 13 stat04"

$slave8
[1] "I am 8 of 13 stat04"

$slave9
[1] "I am 9 of 13 stat04"

$slave10
[1] "I am 10 of 13 stat04"

$slave11
[1] "I am 11 of 13 stat04"

$slave12
[1] "I am 12 of 13 stat04"

[1] 1

[Rmpi 기본 틀] -> 이거는 확실하지 X
(코드)
library(Rmpi)
print(paste(mpi.comm.rank(0), Sys.info()["nodename"]))
mpi.quit()
(결과) -> 반드시 np 사용!(-np 자체가 mpi.spawn.Rslaves()의 역할을 대신함!!!)
user@stathdfs05:~$ mpirun -np 8 --hostfile host_file R --slave -f /home/user/test2.R
[1] "0 stathdfs05"
[1] "1 stathdfs05"
[1] "2 stathdfs05"
[1] "3 stathdfs05"
[1] "4 stat04"
[1] "5 stat04"
[1] "6 stat04"
[1] "7 stat04"

[성공한 예제] -> openmpi의 정상적인 설치 확인 가능 예제
user@stathdfs05:~$ mpicc -o hello hello.c
user@stathdfs05:~$ mpirun -np 2 -H stathdfs05,stat04 ./hello
Hello world from processor stathdfs05, rank 0 out of 2 processors
Hello world from processor stat04, rank 1 out of 2 processors

[6번 해결]
sudo vi host_file(임의의 이름 가능)
(파일 내용)
stathdfs05 slots=5 (master용 cpu 1개를 여분으로 남긴다)
stat04 slots=4
	-> 각각 최대 4개씩의 cpu를 구동할 수 있게 된다
user@stathdfs05:~$ mpicc -o hello hello.c
user@stathdfs05:~$ mpirun -n 8 --hostfile host_file ./hello
(결과)
Hello world from processor stathdfs05, rank 0 out of 8 processors
Hello world from processor stathdfs05, rank 1 out of 8 processors
Hello world from processor stathdfs05, rank 2 out of 8 processors
Hello world from processor stathdfs05, rank 3 out of 8 processors
Hello world from processor stat04, rank 4 out of 8 processors
Hello world from processor stat04, rank 5 out of 8 processors
Hello world from processor stat04, rank 6 out of 8 processors
Hello world from processor stat04, rank 7 out of 8 processors

[11번]
(예제)
y = 1:300
ans0 = sum(y)
print(paste("the actual sum is", ans0))
(결과)
user@stathdfs05:~$ mpirun --hostfile host_file -n 8 R --slave -f /home/user/sum_mpi.R
[1] "the actual sum is 45150"
[1] "the actual sum is 45150"
[1] "the actual sum is 45150"
[1] "the actual sum is 45150"
[1] "the actual sum is 45150"
[1] "the actual sum is 45150"
[1] "the actual sum is 45150"
[1] "the actual sum is 45150"

user@stathdfs05:~$ mpirun --hostfile host_file -n 8 R --slave -f /home/user/test.R
[1] 8
[1] 4
[1] 1
[1] 1
[1] 2
[1] 3
[1] 1
[1] 4

